ADAPTIVE SURVEILLANCE: DETECTING SPECIFIC OBJECT IN VIDEO STREAM USING NEURAL NETWORKDescriptionThis project details the design and implementation of an adaptive CCTV surveillance system that utilizes Convolutional Neural Networks (CNNs) for object detection. The system features a user-adaptable framework, engineered to provide precise recording of events and to be tailored for unique surveillance needs. A central feature is the User-Configurable Object Identification module, which allows users to upload images of targets of interest to train a personalized CNN model. To enhance the clarity of objects, minimize extraneous visual data, and improve the accuracy of subsequent identification processes, the system incorporates video preprocessing through OpenCV-based filtering methods. The system demonstrates versatility and dependability in realistic surveillance contexts.  The system is designed to address the limitations of manual review of CCTV footage, offering a scalable, automated solution for law enforcement and security sectors.Table of ContentsInstallationUsageModules3.1 Image Augmentation Module3.2 Object Detection Module (CNN)3.3 Video Processing Module3.4 User-Configurable Object Identification Module3.5 Frame Capture Module3.6 Annotation ModuleTechnologies UsedDatasetsResults (Expected)Future WorkAcknowledgmentsLicense1. InstallationPrerequisitesPython 3.xpip (Python package installer)DependenciesA requirements.txt file should be included in the repository with the following (and any other dependencies):opencv-python
tensorflow # Or tensorflow-gpu, if a GPU is available
keras #  if used as an API for tensorflow
torch # If PyTorch is used
torchvision # If PyTorch is used
numpy
matplotlib
To install the dependencies:pip install -r requirements.txt
2. UsageProvide instructions on how to run the system.  This should include:How to perform image augmentation.How to perform object detection on images and video.How to use the user-configurable module.How to use the frame capture moduleHow to use the annotation moduleAny necessary setup or configuration steps.Example commands.For example:#   Run the image augmentation:
python image_augmentation.py

#   Run the object detection on a directory of images:
python object_detection.py --image_dir <path_to_image_directory> --output_dir <path_to_output_directory>

#   Run the object detection on a video:
python object_detection.py --video_path <path_to_video_file> --output_dir <path_to_output_directory>

#   Train the object detection model:
python train_model.py

#   Run the frame capture module:
python frame_capture.py

#   Run the annotation module
python annotation.py
3. Modules3.1 Image Augmentation ModuleUses tensorflow.keras.preprocessing.image.ImageDataGenerator for image augmentation.Functionality:Applies random transformations to images, including rotation, shifting, shearing, zooming, and horizontal flipping.Normalizes pixel values.Saves augmented images to a specified directory.This module is used to increase the variability of the training data, which can improve the robustness and generalization of the object detection model.3.2 Object Detection Module (CNN)Implements object detection using CNNs.Architecture: Faster R-CNN with ResNet-50 backbone.Frameworks: PyTorch and/or TensorFlow/Keras.Functionality:Detects objects (specifically cars) in images and video frames.Filters detections based on a confidence threshold.Visualizes detections with bounding boxes and labels.Saves annotated images and video frames.The module processes pre-processed video frames, outputting bounding boxes and confidence scores for detected objects. This user-trainable CNN forms the core of the object detection capability.3.3 Video Processing ModuleHandles video input and preprocessing.Technology: OpenCV.Functionality:Reads video streams from sources such as CCTV cameras or video files.Performs video decoding, frame extraction, and essential preprocessing.Enhances object clarity and reduces noise using techniques like noise reduction and image enhancement.Delivers the processed frames to the Object Detection Module.This module is crucial for optimizing the performance and accuracy of the object detection phase.3.4 User-Configurable Object Identification ModuleAllows users to define custom objects for detection.Functionality:Users upload images of target objects.The system trains a personalized CNN model based on the uploaded images.The custom model is integrated into the object detection pipeline.This module empowers the system to recognize specific targets tailored to individual needs.3.5 Frame Capture ModuleDesigned to autonomously isolate and store video frames featuring pre-selected objects.Functionality:Triggered by the system's CNN-driven Object Detection Module.Retrieves relevant video frames, potentially from the Video Processing Module.Saves captured frames to a defined location using a structured naming convention.This module aims to significantly lessen the need for manual review of extensive video footage and accelerate investigative processes.3.6 Annotation ModuleFocuses on creating a high-quality annotated dataset for training the object detection model.Functionality:Processes user-provided images that have undergone augmentation.Employs a pre-trained Faster R-CNN model as an annotation assistant to suggest bounding boxes.Allows human annotators to review and adjust the proposed bounding boxes and labels.Outputs a comprehensive annotated dataset of augmented images with accurate bounding box information.This semi-automated approach reduces the time and effort required for manual annotation and enhances the precision of the annotations.4. Technologies UsedPythonOpenCVTensorFlow/Keras (If used)PyTorch (If used)Faster R-CNNResNet-50Matplotlib5. DatasetsSpecify the datasets used for training and testing.  If using standard datasets, provide links.  For example:COCO Dataset: https://cocodataset.org/ (If used)Custom Dataset:  Describe the custom dataset, if any, and how it was collected.6. Results (Expected)Summarize the expected performance of the system.  If available, include:Performance metrics (e.g., mAP, Precision, Recall, F1-score).Qualitative results (e.g., sample detections).A link to a more detailed report, if available.7. Future WorkOutline potential future improvements or extensions:Optimization of the system for real-time performance.Integration of object tracking.Expansion of the user interface.Deployment on specific hardware.8. AcknowledgmentsMs.T Archana, AP/CSE, Department of Computer Science and Engineering, SRM Institute of Science and Technology, for her invaluable guidance and support throughout this research project.9. LicenseNo license has been specified for this project.
