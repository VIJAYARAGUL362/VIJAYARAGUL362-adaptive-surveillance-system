{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae3808a-70ce-405f-b506-74c6ff3bbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        rescale =1./255    \n",
    ")\n",
    "\n",
    "img = load_img('real_car_image1.jpg',target_size=(700,700))  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='car', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6894c8-3652-40e7-a8cb-d9d4aa137392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car annotations saved to: annotations\n",
      "Annotated images saved to: annotated images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# COCO category names\n",
    "COCO_CATEGORIES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock',\n",
    "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "CAR_LABEL = 'car'\n",
    "CAR_CATEGORY_INDEX = COCO_CATEGORIES.index(CAR_LABEL)\n",
    "\n",
    "image_dir ='preview'\n",
    "output_annotated_dir='annotations'\n",
    "output_annotated_image_dir='annotated images'\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "def auto_annotate_directory_faster_rcnn_car_only(image_dir, output_annot_dir, output_annotated_image_dir, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Automatically annotates cars in all images within a directory using Faster R-CNN.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing images.\n",
    "        output_annot_dir (str): Path to the directory to save annotation text files.\n",
    "        output_annotated_image_dir (str): Path to directory to save annotated images (optional, can be None).\n",
    "        confidence_threshold (float): Confidence threshold for car detections.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_annot_dir):\n",
    "        os.makedirs(output_annot_dir)\n",
    "    if output_annotated_image_dir and not os.path.exists(output_annotated_image_dir):\n",
    "        os.makedirs(output_annotated_image_dir)\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        annotated_image_rgb, annotations = auto_annotate_faster_rcnn_car_only_single_image(image_path, confidence_threshold)\n",
    "\n",
    "        # Save annotations to a text file (one per image)\n",
    "        annot_file_name = os.path.splitext(image_file)[0] + \".txt\" # Same name as image, but .txt extension\n",
    "        annot_file_path = os.path.join(output_annot_dir, annot_file_name)\n",
    "        with open(annot_file_path, 'w') as f:\n",
    "            for ann in annotations:\n",
    "                bbox = ann['bbox']\n",
    "                confidence = ann['confidence']\n",
    "                line = f\"{CAR_LABEL} {confidence:.4f} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\" # Format: label confidence x_min y_min x_max y_max\n",
    "                f.write(line)\n",
    "\n",
    "        if output_annotated_image_dir:\n",
    "            annotated_image_output_path = os.path.join(output_annotated_image_dir, image_file)\n",
    "            cv2.imwrite(annotated_image_output_path, cv2.cvtColor(annotated_image_rgb, cv2.COLOR_RGB2BGR)) # Save annotated image\n",
    "\n",
    "def auto_annotate_faster_rcnn_car_only_single_image(image_path, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Annotates cars in a single image using Faster R-CNN, keeping only the detection with the highest confidence.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = transforms.ToTensor()(image_rgb).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "\n",
    "    boxes = prediction[0]['boxes'].cpu()\n",
    "    labels = prediction[0]['labels'].cpu()\n",
    "    scores = prediction[0]['scores'].cpu()\n",
    "\n",
    "    # --------------------  INSERT THE CODE BLOCK BELOW HERE  --------------------\n",
    "    # REPLACE the original 'annotated_image = image.copy()...' and the original 'for' loop\n",
    "    annotated_image = image.copy()\n",
    "    annotations = []\n",
    "    highest_confidence = -1.0 # Initialize to a very low value\n",
    "    best_detection = None\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        score = scores[i]\n",
    "        label_id = labels[i]\n",
    "        if label_id == CAR_CATEGORY_INDEX and score > confidence_threshold:\n",
    "            confidence = float(score.numpy())\n",
    "            if confidence > highest_confidence: # Check if current detection has higher confidence\n",
    "                highest_confidence = confidence\n",
    "                best_detection = { # Store the detection with highest confidence so far\n",
    "                    'bbox': box.numpy().astype(int),\n",
    "                    'label': CAR_LABEL,\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "\n",
    "    if best_detection: # If a best detection was found (at least one car above threshold)\n",
    "        box = best_detection['bbox']\n",
    "        cv2.rectangle(annotated_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        caption = f\"{CAR_LABEL} ({best_detection['confidence']:.2f})\"\n",
    "        cv2.putText(annotated_image, caption, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        annotations = [best_detection] # Create annotation list with only the best detection\n",
    "    # --------------------  END OF INSERTION  --------------------\n",
    "\n",
    "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    return annotated_image_rgb, annotations\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for a directory of images:\n",
    "# image_directory = 'path/to/your/image/directory'  # Replace with path to your directory of images\n",
    "# output_annotation_directory = 'output_annotations' # Directory to save annotation text files\n",
    "# output_annotated_images_directory = 'output_annotated_images' # Directory to save images with bounding boxes (optional, set to None to skip)\n",
    "\n",
    "auto_annotate_directory_faster_rcnn_car_only(image_dir, output_annotated_dir, output_annotated_image_dir, confidence_threshold=0.3)\n",
    "\n",
    "print(f\"Car annotations saved to: {output_annotated_dir}\")\n",
    "if output_annotated_image_dir:\n",
    "    print(f\"Annotated images saved to: {output_annotated_image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db82f5-8cd4-45c7-889d-1383da588f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 100 images in preview\n",
      "Starting training for 10 epochs...\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Epoch 1/10, Loss: 0.2231, Time: 1658.47s\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Original image type: <class 'PIL.Image.Image'>\n",
      "Transformed image type: <class 'torch.Tensor'>\n",
      "Transformed image shape: torch.Size([3, 700, 700])\n",
      "Batch images type: <class 'torch.Tensor'>\n",
      "Batch images shape: torch.Size([3, 700, 700])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize(   # Normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class CarDetectionDataset(Dataset):\n",
    "    \"\"\"Custom dataset for car detection with text annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, annotations_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with images\n",
    "            annotations_dir (str): Directory with annotation text files\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        \n",
    "        # Sort to ensure consistent ordering\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {image_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        print(f\"Original image type: {type(image)}\")  # Debug: Should be <class 'PIL.Image.Image'>\n",
    "        \n",
    "        # Get annotation filename\n",
    "        annotation_file = os.path.splitext(self.image_files[idx])[0] + \".txt\"\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_file)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # Parse annotation file\n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 6:\n",
    "                        class_name = parts[0]\n",
    "                        confidence = float(parts[1])\n",
    "                        x_min = int(float(parts[2]))\n",
    "                        y_min = int(float(parts[3]))\n",
    "                        x_max = int(float(parts[4]))\n",
    "                        y_max = int(float(parts[5]))\n",
    "                        \n",
    "                        # Add bounding box\n",
    "                        boxes.append([x_min, y_min, x_max, y_max])\n",
    "                        # Class label (assuming 'car' is class 1)\n",
    "                        labels.append(1)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        # Calculate areas\n",
    "        if len(boxes) > 0:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.tensor([])\n",
    "        \n",
    "        # Suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            print(f\"Transformed image type: {type(image)}\")  # Debug: Should be <class 'torch.Tensor'>\n",
    "            print(f\"Transformed image shape: {image.shape}\")  # Debug: Should be [C, H, W]\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Use a custom collate function to handle batches\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    print(f\"Collated images type: {type(images[0])}\")  # Debug: Should be <class 'torch.Tensor'>\n",
    "    print(f\"Collated images shape: {images[0].shape}\")  # Debug: Should be [C, H, W]\n",
    "    return images, targets\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Faster R-CNN model and replace the classification head \n",
    "    with a new one for our number of classes\n",
    "    \"\"\"\n",
    "    # Load a pre-trained model\n",
    "    model = fasterrcnn_resnet50_fpn()\n",
    "    \n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, data_loader, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            print(f\"Batch images type: {type(images[0])}\")  # Debug: Should be <class 'torch.Tensor'>\n",
    "            print(f\"Batch images shape: {images[0].shape}\")  # Debug: Should be [C, H, W]\n",
    "            \n",
    "            # Move images and targets to the correct device\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += losses.item()\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(data_loader):.4f}, Time: {time_taken:.2f}s\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "def process_video(model, video_path, output_dir, confidence_threshold=0.5, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Process a video, detect objects, and save frames with detections\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Faster R-CNN model\n",
    "        video_path: Path to the video file\n",
    "        output_dir: Directory to save frames with detections\n",
    "        confidence_threshold: Minimum confidence score to consider a detection\n",
    "        device: Device to run the model on (cpu or cuda)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video info: {frame_width}x{frame_height}, {fps} fps, {total_frames} total frames\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    frame_count = 0\n",
    "    detected_frames = 0\n",
    "    \n",
    "    # Process each frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB and then to tensor\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transforms.ToTensor()(frame_rgb).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)\n",
    "        \n",
    "        # Get boxes, labels, and scores\n",
    "        boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "        labels = prediction[0]['labels'].cpu().numpy()\n",
    "        scores = prediction[0]['scores'].cpu().numpy()\n",
    "        \n",
    "        # Filter detections based on confidence threshold\n",
    "        mask = scores >= confidence_threshold\n",
    "        boxes = boxes[mask]\n",
    "        labels = labels[mask]\n",
    "        scores = scores[mask]\n",
    "        \n",
    "        # Save frame if any detections\n",
    "        if len(boxes) > 0:\n",
    "            detected_frames += 1\n",
    "            \n",
    "            # Create output filename\n",
    "            output_filename = f\"frame_{frame_count:06d}.jpg\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Draw bounding boxes on frame\n",
    "            annotated_frame = frame.copy()\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label_text = f\"Car: {score:.2f}\"\n",
    "                cv2.putText(annotated_frame, label_text, (x1, y1 - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # Save annotated frame\n",
    "            cv2.imwrite(output_path, annotated_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%)\")\n",
    "            print(f\"Detected objects in {detected_frames} frames so far\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Video processing complete. Found objects in {detected_frames} out of {frame_count} frames.\")\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    image_dir = \"preview\"                # Directory containing training images\n",
    "    annotations_dir = \"annotations\"      # Directory containing annotation text files\n",
    "    video_path = \"car_vidieo_2.mp4\"        # Path to the video file to process\n",
    "    output_dir = \"detected_frames\"       # Directory to save frames with detections\n",
    "    model_save_path = \"car_detection_model_1.pth\"  # Path to save the trained model\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Training parameters\n",
    "    num_classes = 2  # Background + Car\n",
    "    batch_size = 2\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CarDetectionDataset(image_dir, annotations_dir, transform=transform)\n",
    "    \n",
    "    # Use a custom collate function to handle batches\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Get model\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    \n",
    "    # Define optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(model, data_loader, optimizer, device, num_epochs=num_epochs)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Process video\n",
    "    process_video(model, video_path, output_dir, confidence_threshold=0.5, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fecf2-a996-4824-854d-6853ae4a0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, \n",
    "    average_precision_score, \n",
    "    confusion_matrix, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score\n",
    ")\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for model evaluation with ground truth annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, annotations_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images for evaluation\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image {img_path}\")\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        annotation_file = os.path.splitext(self.image_files[idx])[0] + \".txt\"\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_file)\n",
    "        \n",
    "        gt_boxes = []\n",
    "        gt_labels = []\n",
    "        \n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 6:\n",
    "                        x_min = int(float(parts[2]))\n",
    "                        y_min = int(float(parts[3]))\n",
    "                        x_max = int(float(parts[4]))\n",
    "                        y_max = int(float(parts[5]))\n",
    "                        \n",
    "                        gt_boxes.append([x_min, y_min, x_max, y_max])\n",
    "                        gt_labels.append(1)  # Car class\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, {\n",
    "            'boxes': torch.tensor(gt_boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(gt_labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def evaluate_model(model, data_loader, device, iou_threshold=0.5, conf_thresholds=[0.3, 0.5, 0.7]):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    # Evaluate at multiple confidence thresholds\n",
    "    for conf_threshold in conf_thresholds:\n",
    "        all_true_positives = []\n",
    "        all_predictions = []\n",
    "        all_ground_truth = []\n",
    "        all_scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in data_loader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                predictions = model(images)\n",
    "                \n",
    "                for i, pred in enumerate(predictions):\n",
    "                    # Filter predictions\n",
    "                    mask = pred['scores'] >= conf_threshold\n",
    "                    pred_boxes = pred['boxes'][mask].cpu()\n",
    "                    pred_scores = pred['scores'][mask].cpu()\n",
    "                    \n",
    "                    gt_boxes = targets[i]['boxes']\n",
    "                    \n",
    "                    # Compute matches\n",
    "                    image_preds = []\n",
    "                    image_gt = []\n",
    "                    \n",
    "                    for pred_box, pred_score in zip(pred_boxes, pred_scores):\n",
    "                        # Check if prediction matches any ground truth box\n",
    "                        is_match = any(\n",
    "                            calculate_iou(pred_box, gt_box) >= iou_threshold \n",
    "                            for gt_box in gt_boxes\n",
    "                        )\n",
    "                        \n",
    "                        image_preds.append(1 if is_match else 0)\n",
    "                        image_gt.append(1 if len(gt_boxes) > 0 else 0)\n",
    "                        all_scores.append(pred_score.item())\n",
    "                    \n",
    "                    all_true_positives.extend(image_preds)\n",
    "                    all_predictions.extend(image_preds)\n",
    "                    all_ground_truth.extend(image_gt)\n",
    "        \n",
    "        # Compute metrics\n",
    "        precision = precision_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "        recall = recall_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "        f1 = f1_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "        \n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(all_ground_truth, all_scores)\n",
    "        avg_precision = average_precision_score(all_ground_truth, all_scores)\n",
    "        \n",
    "        evaluation_results[conf_threshold] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'average_precision': avg_precision,\n",
    "            'precision_curve': precision_curve,\n",
    "            'recall_curve': recall_curve\n",
    "        }\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def plot_results(evaluation_results):\n",
    "    \"\"\"Create comprehensive visualization of evaluation results\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Precision-Recall Curves\n",
    "    plt.subplot(2, 3, 1)\n",
    "    for threshold, results in evaluation_results.items():\n",
    "        plt.plot(\n",
    "            results['recall_curve'], \n",
    "            results['precision_curve'], \n",
    "            label=f'Threshold {threshold} (AP={results[\"average_precision\"]:.2f})'\n",
    "        )\n",
    "    plt.title('Precision-Recall Curves')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Bar Plot of Metrics\n",
    "    plt.subplot(2, 3, 2)\n",
    "    thresholds = list(evaluation_results.keys())\n",
    "    precisions = [results['precision'] for results in evaluation_results.values()]\n",
    "    recalls = [results['recall'] for results in evaluation_results.values()]\n",
    "    f1_scores = [results['f1_score'] for results in evaluation_results.values()]\n",
    "    \n",
    "    x = np.arange(len(thresholds))\n",
    "    width = 0.25\n",
    "    plt.bar(x - width, precisions, width, label='Precision')\n",
    "    plt.bar(x, recalls, width, label='Recall')\n",
    "    plt.bar(x + width, f1_scores, width, label='F1 Score')\n",
    "    plt.title('Metrics at Different Thresholds')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, thresholds)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 3. Heatmap of IoU Distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    iou_values = np.random.rand(10, 10)  # Placeholder, replace with actual IoU calculations\n",
    "    sns.heatmap(iou_values, cmap='YlGnBu', annot=True, fmt='.2f')\n",
    "    plt.title('IoU Distribution')\n",
    "    \n",
    "    # 4. Performance Radar Chart\n",
    "    plt.subplot(2, 3, 4, polar=True)\n",
    "    metrics = [\n",
    "        evaluation_results[0.5]['precision'], \n",
    "        evaluation_results[0.5]['recall'], \n",
    "        evaluation_results[0.5]['f1_score'],\n",
    "        evaluation_results[0.5]['average_precision']\n",
    "    ]\n",
    "    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "    metrics += metrics[:1]\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    plt.polar(angles, metrics, 'o-', linewidth=2)\n",
    "    plt.fill(angles, metrics, alpha=0.25)\n",
    "    plt.title('Performance Radar Chart')\n",
    "    plt.xticks(angles[:-1], ['Precision', 'Recall', 'F1', 'AP'])\n",
    "    \n",
    "    # 5. Average Precision Comparison\n",
    "    plt.subplot(2, 3, 5)\n",
    "    aps = [results['average_precision'] for results in evaluation_results.values()]\n",
    "    plt.bar(thresholds, aps)\n",
    "    plt.title('Average Precision')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Average Precision')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation_results.png', dpi=300)\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    image_dir = \"preview\"\n",
    "    annotations_dir = \"annotations\"\n",
    "    model_path = \"car_detection_model_1.pth\"\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file {model_path} not found!\")\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Evaluation device: {device}\")\n",
    "    \n",
    "    # Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = EvaluationDataset(image_dir, annotations_dir, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=2, shuffle=False, \n",
    "                              collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    # Load model\n",
    "    num_classes = 2  # Background + Car\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluation_results = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    # Plot comprehensive results\n",
    "    plot_results(evaluation_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    for threshold, metrics in evaluation_results.items():\n",
    "        print(f\"\\nConfidence Threshold: {threshold}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"Average Precision: {metrics['average_precision']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db80e44-3575-48d6-bf98-922f59690494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc83d6e-63c8-4270-9d74-7feea4c30e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fa123-d204-4144-a00b-5118adb9bf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
